{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a17f07e-e09c-4fba-9b41-87ffe1d7bfaa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-23T03:00:38.543362200Z",
     "start_time": "2023-06-23T03:00:34.877714700Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tifffile as tif\n",
    "import os\n",
    "\n",
    "# data manager and analysis\n",
    "import vodex as vx\n",
    "import numan as nu\n",
    "import pandas as pd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fcf69bf2",
   "metadata": {},
   "source": [
    "Get the project directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba63983-c186-45a2-8e24-913465354f12",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-23T03:00:40.085886600Z",
     "start_time": "2023-06-23T03:00:40.079886Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# please provedi FULL, not relative, path to the folder \n",
    "project_folder = \"/home/ply/repos/numan_dev/numan/data/hz09\"\n",
    "project = nu.Project(project_folder)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cf7bad9f-c4ad-4628-b979-0a0a828fa630",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We will also set the processed directory as our working directory, this step is important , since all the paths later are relative to this folder. Verify that the output of the cell is the \"processed\" folder inside your project folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d736f2-2ade-41f1-87b2-132cc0dab2eb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-23T03:00:42.600462700Z",
     "start_time": "2023-06-23T03:00:42.545737300Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "project.activate(\"processed\")\n",
    "os.getcwd()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a4273882-09e6-48a1-ae05-18a63f5b5f70",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Load experiment: \n",
    "\n",
    "* For now we still need the raw experiment since we will be loading data around from it to process. \n",
    "* we will also need the information about the truncated experiment that we saved to stimuli_truncated_timelines.csv in the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa59ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = vx.Experiment.load(\"experiment_raw.db\")\n",
    "\n",
    "# grab the volumes around the stimuli only:\n",
    "stimuli_volumes_df = pd.read_csv(\"stimuli_truncated_timelines.csv\", index_col = False)\n",
    "stimuli_volumes_extended = stimuli_volumes_df[\"volumes\"].to_numpy()\n",
    "\n",
    "stimuli_volumes_extended[0:9]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a266b3b6",
   "metadata": {},
   "source": [
    "From now on we will only use the volumes that we defined above. This will save us a lot of computation time."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "777e94dd",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Denoising\n",
    "\n",
    "You need to provide the offset file for denoising"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8feebbe6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-23T03:01:12.154261600Z",
     "start_time": "2023-06-23T03:01:12.134231100Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# this file is the same for all experiments with this camera settings\n",
    "# you can get the file at https://github.com/LemonJust/numan/tree/main/notebooks/data/denoising_2023_hz\n",
    "offset_file = \"/home/ply/repos/numan_dev/numan/notebooks/data/denoising_2023_hz/calibration_offset.tif\"\n",
    "\n",
    "# 200 volumes takes about 12 Gb of RAM and takes about 10 minutes to process, \n",
    "# but 1 file with 200 volumes is about 4Gb , \n",
    "# so shouldn't increase it above 200\n",
    "batch_size = 200 # in volumes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-23T03:10:42.857663200Z",
     "start_time": "2023-06-23T03:01:15.163373800Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# takes about 3 minutes to process on beefsy\n",
    "project.create(\"processed/removed_offset\")\n",
    "\n",
    "nu.Preprocess(experiment).remove_offset('removed_offset',batch_size, volumes = stimuli_volumes_extended, offset_file = offset_file,\n",
    "                                        verbose=True )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c1e6b1f1",
   "metadata": {},
   "source": [
    "Update the experiment to use the denoised files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-23T03:17:25.734601300Z",
     "start_time": "2023-06-23T03:17:23.658146900Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# update files info to use the new removed_offset folder instead of the original raw data\n",
    "data_dir = \"removed_offset\"\n",
    "\n",
    "# volumes info is the same as in the original experiment\n",
    "frames_per_volume = experiment.frames_per_volume\n",
    "starting_slice = experiment.starting_slice\n",
    "\n",
    "# close the old experiment\n",
    "experiment.close()\n",
    "# create new experiment\n",
    "experiment = vx.Experiment.from_dir(data_dir, frames_per_volume, starting_slice, verbose=True)\n",
    "# add time annotation \n",
    "experiment.add_annotations_from_volume_annotation_df(stimuli_volumes_df)\n",
    "\n",
    "# for now there will be no description of the labels when adding them from the volume annotation df\n",
    "# so you will see description : None for all labels in the labels_df\n",
    "# will add this option in later versions of vodex\n",
    "experiment.labels_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8066a572",
   "metadata": {},
   "source": [
    "If everything looks good, save the experiment to the processed folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f35d118",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment.save(\"experiment_truncated_denoised.db\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Drift correction\n",
    "* will only register the volumes that we defined above for the truncated experiment.\n",
    "* uses denoised data\n",
    "* for now registers to the 1000th volume in the movie, might add a smarter way to pick the template later\n",
    "* using Affne registration\n",
    "* takes about 2.5 hours on beefsy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392ab5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = vx.Experiment.load(\"experiment_truncated_denoised.db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beae7f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = \"drift_corrected_af_to1000\"\n",
    "project.create(f\"processed/{save_dir}\")\n",
    "\n",
    "# the RAM doesn't depend much on the batch size (needs baout 4 - 5 Gb) as each volume pair is loaded separately for the registration\n",
    "# takes about 14 min for 200 volumes, so 2.5 hours for the whole experiment (2160 volumes)\n",
    "batch_size = 200 # in volumes\n",
    "template = 1000 # approx in the middle of the truncated experiment\n",
    "spacing_xyz = [1.62, 1.62, 3.83] # in um\n",
    "\n",
    "nu.Preprocess(experiment).drift_correct_naive(save_dir,batch_size, spacing_xyz, registration_type = \"Affine\", template = template, verbose=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b60848f0",
   "metadata": {},
   "source": [
    "Update the experiment to use the drift corrected files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb216402",
   "metadata": {},
   "outputs": [],
   "source": [
    "# update files info to use the new removed_offset folder instead of the original raw data\n",
    "data_dir = \"drift_corrected_af_to1000\"\n",
    "\n",
    "# volumes info is the same as in the original experiment\n",
    "frames_per_volume = experiment.frames_per_volume\n",
    "starting_slice = experiment.starting_slice\n",
    "\n",
    "# close the old experiment\n",
    "experiment.close()\n",
    "# create new experiment\n",
    "experiment = vx.Experiment.from_dir(data_dir, frames_per_volume, starting_slice, verbose=True)\n",
    "# add time annotation \n",
    "experiment.add_annotations_from_volume_annotation_df(stimuli_volumes_df)\n",
    "\n",
    "# for now there will be no description of the labels when adding them from the volume annotation df\n",
    "# so you will see description : None for all labels in the labels_df\n",
    "# will add this option in later versions of vodex\n",
    "experiment.labels_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3e0a97b3",
   "metadata": {},
   "source": [
    "If everything looks good, save the experiment to the processed folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4db846",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment.save(\"experiment_truncated_drift_corrected.db\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0b49b7e2",
   "metadata": {},
   "source": [
    "Save three slices for easy visualization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef7e71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create directory for slices\n",
    "project.create(\"processed/drift_corrected_slices\")\n",
    "\n",
    "# get the slices\n",
    "# you can pick any slices you want, \n",
    "# they will be saved in the same file\n",
    "# the order doesn't matter (vodex will load them smallest to largest)\n",
    "# three slices are about 4 Gb for the whole movie\n",
    "slices_id = [10, 17, 22]\n",
    "slices = experiment.load_slices(slices_id)\n",
    "\n",
    "# create file name according to the slices id\n",
    "file_name = \"drift_corrected_slices/dc_slices\"\n",
    "for slice_id in slices_id:\n",
    "    file_name = file_name + \"_\" + str(slice_id)\n",
    "\n",
    "# save the slices\n",
    "tif.imwrite(file_name + \".tif\",\n",
    "            slices.astype(np.int16), \n",
    "            shape=slices.shape, metadata={'axes': 'TZYX'}, imagej=True)\n",
    "\n",
    "experiment.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "numan_dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
